---
layout: paper
title: "ShadowSense: Detecting Human Touch in a Social Robot Using Shadow Image Classification"
year: 2020
shortref: "Hu et al. IMWUT 2020"
shorttitle: "ShadowSense: Social Robot Touch Detection Using Image Classification"
nickname: shadowsense
journal: "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
volume: 4
issue: 4
pages: 1-24
authors: "Hu, Y., Bejarano, S. M., & Hoffman, G."
image: shadowsense.jpg
pdf: HuetalIMWUT20.pdf
pdflink:
fulltext:
pubtype: journal
github:
pmid:
pmcid:
f1000:
figshare:
doi: 10.1145/3432202
category: papers
published: true
tags: [ai, perception, touch]
---
{% include JB/setup %}

# Abstract

This paper proposes and evaluates the use of image classification for detailed, full-body human-robot tactile interaction. A camera positioned below a translucent robot skin captures shadows generated from human touch and infers social gestures from the captured images. This approach enables rich tactile interaction with robots without the need for the sensor arrays used in traditional social robot tactile skins. It also supports the use of touch interaction with non-rigid robots, achieves high-resolution sensing for robots with different sizes and shape of surfaces, and removes the requirement of direct contact with the robot. We demonstrate the idea with an inflatable robot and a standing-alone testing device, an algorithm for recognizing touch gestures from shadows that uses Densely Connected Convolutional Networks, and an algorithm for tracking positions of touch and hovering shadows. Our experiments show that the system can distinguish between six touch gestures under three lighting conditions with 87.5 - 96.0% accuracy, depending on the lighting, and can accurately track touch positions as well as infer motion activities in realistic interaction conditions. Additional applications for this method include interactive screens on inflatable robots and privacy-maintaining robots for the home.

